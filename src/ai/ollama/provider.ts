import { ChatCompletionTool } from "openai/resources";
import { Recorder } from "../../recorder/recorder.js";
import { Chat, getImages, getTextAndInstructions } from "../chat.js";
import {
  AIProvider,
  AIRequest,
  AIResponse,
  StopReason,
  ToolCall,
} from "../types.js";
import { OllamaMessage, OllamaRequest, OllamaSystemMessage } from "./types.js";

const DEFAULT_OLLAMA_URL = "http://localhost:11434";

export class OllamaProvider implements AIProvider {
  name = "Ollama";
  url: string;
  model: string;
  recorder?: Recorder;

  constructor(model: string, url?: string) {
    this.url = url || DEFAULT_OLLAMA_URL;
    this.model = model;
  }

  createChatRequest(
    chat: Chat,
    context: { recorder?: Recorder } = {},
  ): AIRequest {
    const { recorder } = context;
    if (chat.hasFiles()) {
      recorder?.warn?.log(
        `Ollama model ${this.model} multimodal support depends on the specific model. Ensure you're using a vision-capable model like llava.`,
      );
    }
    return new OllamaChatCompletionRequest(this.url, this.model, chat);
  }
}

class OllamaChatCompletionRequest implements AIRequest {
  chat: Chat;
  url: string;
  model: string;

  constructor(url: string, model: string, chat: Chat) {
    this.url = url;
    this.model = model;
    this.chat = chat;
  }

  async execute(runtime: { recorder?: Recorder }): Promise<AIResponse> {
    const { recorder } = runtime;
    const requestBody = {
      stream: false,
      options: {
        temperature: 0.7,
      },
      ...prepareRequest(this.chat, this.model),
    };

    recorder?.debug?.log(requestBody);

    let result: AIResponse;
    try {
      const response = await fetch(`${this.url}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        console.log(response);
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      result = translateResponse(data);
    } catch (e) {
      recorder?.error?.log("Error fetching Ollama response:", e);
      result = {
        type: "error",
        error: {
          type: "OllamaError",
          message: e.message || "Unexpected error from Ollama",
        },
        usage: {
          in: 0,
          out: 0,
        },
        raw: JSON.stringify(e),
      };
    }

    recorder?.debug?.log(result);
    return result;
  }
}

/**
 * This is similar to the OpenAI format, except the tool call arguments
 * needs to match that generated by the model, which can be an object.
 * @param chat
 * @returns
 */
export function prepareRequest(chat: Chat, model: string): OllamaRequest {
  const systemMsg: OllamaSystemMessage[] = [];
  if (chat.system) {
    systemMsg.push({
      role: "system",
      content: chat.system,
    });
  }

  let tools: ChatCompletionTool[] | undefined = undefined;
  if (chat.tools.length > 0) {
    tools = chat.tools.map((schema) => {
      return {
        type: "function",
        function: schema,
      };
    });
  }

  const messages: OllamaMessage[] = chat.messages
    .map((msg) => {
      if (msg.role === "tool") {
        return msg.content.map((r) => ({
          role: "tool" as const,
          tool_call_id: r.id,
          content: r.content,
        }));
      }

      if (msg.role === "assistant") {
        const toolCalls = msg.toolCalls?.map((call) => {
          const id = call.id;
          return {
            type: "function",
            function: {
              name: call.name,
              arguments: call.arguments,
            },
            ...(id && { id }),
          };
        });
        return {
          role: msg.role,
          content: msg.content,
          ...(toolCalls && { toolCalls }),
        };
      }

      if (typeof msg.content === "string") {
        return {
          role: msg.role,
          content: msg.content,
        };
      } else {
        const content = getTextAndInstructions(msg.content);
        const images = getImages(msg.content).map((img) => img.base64);

        return {
          role: msg.role,
          content,
          ...(images.length > 0 && {
            images: images,
          }),
        };
      }
    })
    .flat(1);

  return {
    model,
    messages: [...systemMsg, ...messages],
    ...(tools && { tools }),
  };
}

function translateResponse(data: any): AIResponse {
  if (data.done_reason === "stop" && data.message) {
    const content = data.message.content;
    const toolCalls: ToolCall[] = [];
    if (data.message.tool_calls) {
      for (const call of data.message.tool_calls) {
        toolCalls.push({
          id: call.id,
          name: call.function.name,
          arguments: call.function.arguments,
        });
      }
    }
    const hasToolCalls = toolCalls.length > 0;

    return {
      type: "success",
      id: `ollama-${Date.now()}`,
      model: data.model,
      reason: hasToolCalls ? StopReason.FunctionCall : StopReason.Stop,
      message: {
        role: "assistant",
        content,
        ...(hasToolCalls && { toolCalls }),
      },
      usage: {
        in: data.prompt_eval_count || 0,
        out: data.eval_count || 0,
      },
      raw: data,
    };
  }

  return {
    type: "error",
    error: {
      type: "OllamaError",
      message: "Unexpected error from Ollama",
    },
    usage: {
      in: 0,
      out: 0,
    },
    raw: data,
  };
}
