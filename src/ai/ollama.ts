import { Recorder } from "../recorder/recorder.js";
import { Chat } from "./chat.js";
import {
  AIProvider,
  AIRequest,
  AIResponse,
  StopReason,
  ToolCall,
} from "./types.js";

const DEFAULT_OLLAMA_URL = "http://localhost:11434";

export class OllamaProvider implements AIProvider {
  name = "Ollama";
  url: string;
  model: string;
  recorder?: Recorder;

  constructor(model: string, url?: string) {
    this.url = url || DEFAULT_OLLAMA_URL;
    this.model = model;
  }

  createChatRequest(
    chat: Chat,
    context: { recorder?: Recorder } = {},
  ): AIRequest {
    const { recorder } = context;
    if (chat.hasFiles()) {
      recorder?.warn?.log(
        `Ollama model ${this.model} multimodal support depends on the specific model. Ensure you're using a vision-capable model like llava.`,
      );
    }
    return new OllamaChatCompletionRequest(this.url, this.model, chat);
  }
}

class OllamaChatCompletionRequest implements AIRequest {
  chat: Chat;
  url: string;
  model: string;

  constructor(url: string, model: string, chat: Chat) {
    this.url = url;
    this.model = model;
    this.chat = chat;
  }

  async execute(runtime: { recorder?: Recorder }): Promise<AIResponse> {
    const { recorder } = runtime;
    const requestBody = {
      model: this.model,
      stream: false,
      options: {
        temperature: 0.7,
      },
      ...prepareRequest(this.chat),
    };

    recorder?.debug?.log(requestBody);

    let result: AIResponse;
    try {
      const response = await fetch(`${this.url}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        console.log(response);
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const data = await response.json();
      result = translateResponse(data);
    } catch (e) {
      recorder?.error?.log("Error fetching Ollama response:", e);
      result = {
        type: "error",
        error: {
          type: "OllamaError",
          message: e.message || "Unexpected error from Ollama",
        },
        usage: {
          in: 0,
          out: 0,
        },
        raw: JSON.stringify(e),
      };
    }

    recorder?.debug?.log(result);
    return result;
  }
}

/**
 * This is similar to the OpenAI format, except the tool call arguments
 * needs to match that generated by the model, which can be an object.
 * @param chat
 * @returns
 */
function prepareRequest(chat: Chat) {
  const systemMsg = [];
  if (chat.system) {
    systemMsg.push({
      role: "system",
      content: chat.system,
    });
  }

  const tools =
    chat.tools.length > 0
      ? chat.tools.map((schema) => {
          return {
            type: "function",
            function: schema,
          };
        })
      : undefined;

  const messages = chat.messages
    .map((msg) => {
      switch (msg.role) {
        case "tool":
          return msg.content.map((r) => ({
            role: "tool",
            tool_call_id: r.id,
            content: r.content,
          }));

        case "assistant":
          return {
            role: msg.role,
            content: msg.content,
            tool_calls: msg.toolCalls.map((call) => {
              const id = call.id;
              return {
                type: "function",
                function: {
                  name: call.name,
                  arguments: call.arguments,
                },
                ...(id && { id }),
              };
            }),
          };

        default:
          if (typeof msg.content === "string") {
            return {
              role: msg.role,
              content: msg.content,
            };
          } else {
            let textContent = "";
            const images: string[] = [];

            for (const item of msg.content) {
              if (item.type === "text") {
                textContent += item.text;
              } else if (item.type === "file") {
                const file = item.file;
                if (file.type === "image") {
                  images.push(file.base64);
                }
              }
            }

            return {
              role: msg.role,
              content: textContent,
              ...(images.length > 0 && { images }),
            };
          }
      }
    })
    .flat(Infinity);

  return {
    messages: [...systemMsg, ...messages],
    ...(tools && { tools }),
  };
}

function translateResponse(data: any): AIResponse {
  if (data.done_reason === "stop" && data.message) {
    const content = data.message.content;
    const toolCalls: ToolCall[] = [];
    if (data.message.tool_calls) {
      for (const call of data.message.tool_calls) {
        toolCalls.push({
          id: call.id,
          name: call.function.name,
          arguments: call.function.arguments,
        });
      }
    }
    const hasToolCalls = toolCalls.length > 0;

    return {
      type: "success",
      id: `ollama-${Date.now()}`,
      model: data.model,
      reason: hasToolCalls ? StopReason.FunctionCall : StopReason.Stop,
      message: {
        role: "assistant",
        content,
        ...(hasToolCalls && { toolCalls }),
      },
      usage: {
        in: data.prompt_eval_count || 0,
        out: data.eval_count || 0,
      },
      raw: data,
    };
  }

  return {
    type: "error",
    error: {
      type: "OllamaError",
      message: "Unexpected error from Ollama",
    },
    usage: {
      in: 0,
      out: 0,
    },
    raw: data,
  };
}
